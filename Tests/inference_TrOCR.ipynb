{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from jiwer import cer  # For CER calculation\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    MODEL_NAME: str = 'microsoft/trocr-small-printed'\n",
    "\n",
    "# Custom Dataset class for handling OCR\n",
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, dictionary_file, max_target_length=100):\n",
    "        self.dictionary = self.load_dictionary(dictionary_file)\n",
    "        self.word_to_id = {word: idx for idx, word in enumerate(self.dictionary)}\n",
    "\n",
    "        # Add special tokens to the dictionary\n",
    "        self.word_to_id[\"<START>\"] = 0\n",
    "        self.word_to_id[\"<END>\"] = 1\n",
    "        self.word_to_id[\"<UNK>\"] = 2\n",
    "        self.word_to_id[\"<PAD>\"] = 3\n",
    "\n",
    "    def load_dictionary(self, tokenized_file):\n",
    "        with open(tokenized_file, 'r', encoding='utf-8') as file:\n",
    "            words = [line.strip() for line in file.readlines()]\n",
    "        special_tokens = [\"<START>\", \"<END>\", \"<UNK>\", \"<PAD>\", \" \"]\n",
    "        return special_tokens + words\n",
    "\n",
    "    def decode_labels(self, labels):\n",
    "        label_str = []\n",
    "        for label in labels:\n",
    "            if label == self.word_to_id[\"<PAD>\"] or label == self.word_to_id[\"<START>\"] or label == self.word_to_id[\"<END>\"]:\n",
    "                continue\n",
    "            elif label == self.word_to_id[\"<UNK>\"]:\n",
    "                label_str.append(\"<UNK>\")\n",
    "            else:\n",
    "                label_str.append(self.dictionary[label])\n",
    "        return \"\".join(label_str)\n",
    "\n",
    "def load_ground_truth(ground_truth_file):\n",
    "    ground_truth = {}\n",
    "    with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            filename, text = line.strip().split(maxsplit=1)\n",
    "            ground_truth[filename] = text\n",
    "    return ground_truth\n",
    "\n",
    "# Load your model configuration and trained model\n",
    "dictionary_file = r\"D:\\Github\\Khmer-OCR\\Experiments\\Dicts\\unique_characters.txt\"\n",
    "custom_dataset = CustomOCRDataset(dictionary_file)\n",
    "\n",
    "# Load the processor and trained model\n",
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trained_model = VisionEncoderDecoderModel.from_pretrained('D:/Github/Khmer-OCR/Experiments/Results/handwritten_v4_fine_tuning_ex_v6/checkpoint-26640').to(device)\n",
    "\n",
    "# Paths\n",
    "image_folder = \"D:/Github/Khmer-OCR/Experiments/Tests/eng_char\"\n",
    "ground_truth_file = r\"D:\\Github\\Khmer-OCR\\Experiments\\Tests\\eng_char.txt\"\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = load_ground_truth(ground_truth_file)\n",
    "\n",
    "# Initialize CER and predictions\n",
    "total_cer = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "# Loop over all images in the folder\n",
    "for image_file in os.listdir(image_folder):\n",
    "    if image_file.endswith(\".jpg\") or image_file.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            generated_ids = trained_model.generate(pixel_values)\n",
    "\n",
    "        # Decode the labels using the custom dataset\n",
    "        predicted_text = custom_dataset.decode_labels(generated_ids[0].cpu().numpy())\n",
    "\n",
    "        # Get ground truth text for the image\n",
    "        ground_truth_text = ground_truth.get(image_file, \"\")\n",
    "\n",
    "        # Check if ground truth is present\n",
    "        if not ground_truth_text:\n",
    "            print(f\"Skipping image {image_file} because ground truth is missing.\")\n",
    "            continue\n",
    "\n",
    "        # Check if predicted text is not empty\n",
    "        if not predicted_text:\n",
    "            print(f\"Skipping image {image_file} because predicted text is empty.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate CER for this sample\n",
    "        sample_cer = cer(ground_truth_text, predicted_text)\n",
    "        total_cer += sample_cer\n",
    "        num_samples += 1\n",
    "\n",
    "        # Output results for this image\n",
    "        print(f\"Image: {image_file}\")\n",
    "        print(f\"Ground Truth: {ground_truth_text}\")\n",
    "        print(f\"Predicted: {predicted_text}\")\n",
    "        print(f\"CER: {sample_cer:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Calculate and display the average CER\n",
    "if num_samples > 0:\n",
    "    avg_cer = total_cer / num_samples\n",
    "    print(f\"Average CER: {avg_cer:.4f}\")\n",
    "else:\n",
    "    print(\"No samples processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
